{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ef11257",
   "metadata": {},
   "source": [
    "Perform Image Augmentation and build custom CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "000d69eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/homebrew/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: torchvision in /opt/homebrew/lib/python3.11/site-packages (0.16.1)\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/lib/python3.11/site-packages (3.8.0)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (4.65.0)\n",
      "Requirement already satisfied: torchsummary in /opt/homebrew/lib/python3.11/site-packages (1.5.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/lib/python3.11/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch) (1.3)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.11/site-packages (from torch) (2023.9.2)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from torchvision) (1.24.4)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/lib/python3.11/site-packages (from torchvision) (10.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchvision) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->torchvision) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision matplotlib tqdm torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b5a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torchsummary import summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "534c4f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e972b7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # using adaptive pooling layer because image sizes are not fixed. \n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1)) \n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05e8c304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x106063db0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be6a5065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations and normalizations\n",
    "\n",
    "if augmentation == False:\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "else:\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "768fff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_train = '../final_dataset/train'\n",
    "data_dir_test = '../final_dataset/test'\n",
    "\n",
    "image_datasets = {\n",
    "    'train': datasets.ImageFolder(root=data_dir_train, transform=data_transforms['train']),\n",
    "    'val': datasets.ImageFolder(root=data_dir_test, transform=data_transforms['val'])\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(image_datasets['train'], batch_size=32, shuffle=True, num_workers=4),\n",
    "    'val': DataLoader(image_datasets['val'], batch_size=32, shuffle=False, num_workers=4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ab114d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2): ReLU()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fb503c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 224, 224]             896\n",
      "              ReLU-2         [-1, 32, 224, 224]               0\n",
      "         MaxPool2d-3         [-1, 32, 112, 112]               0\n",
      "            Conv2d-4         [-1, 64, 112, 112]          18,496\n",
      "              ReLU-5         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-6           [-1, 64, 56, 56]               0\n",
      " AdaptiveAvgPool2d-7             [-1, 64, 1, 1]               0\n",
      "            Linear-8                  [-1, 128]           8,320\n",
      "              ReLU-9                  [-1, 128]               0\n",
      "           Linear-10                    [-1, 2]             258\n",
      "================================================================\n",
      "Total params: 27,970\n",
      "Trainable params: 27,970\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 41.35\n",
      "Params size (MB): 0.11\n",
      "Estimated Total Size (MB): 42.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b28336e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0...\n",
      "train Loss: 0.6832 Acc: 0.5628\n",
      "val Loss: 0.6890 Acc: 0.5613\n",
      "Epoch 1...\n",
      "train Loss: 0.6568 Acc: 0.6129\n",
      "val Loss: 0.6703 Acc: 0.5943\n",
      "Epoch 2...\n",
      "train Loss: 0.6470 Acc: 0.6360\n",
      "val Loss: 0.6602 Acc: 0.6048\n",
      "Epoch 3...\n",
      "train Loss: 0.6444 Acc: 0.6387\n",
      "val Loss: 0.6718 Acc: 0.5907\n",
      "Epoch 4...\n",
      "train Loss: 0.6462 Acc: 0.6324\n",
      "val Loss: 0.6471 Acc: 0.6237\n",
      "Epoch 5...\n",
      "train Loss: 0.6376 Acc: 0.6406\n",
      "val Loss: 0.6428 Acc: 0.6328\n",
      "Epoch 6...\n",
      "train Loss: 0.6262 Acc: 0.6530\n",
      "val Loss: 0.6335 Acc: 0.6384\n",
      "Epoch 7...\n",
      "train Loss: 0.6247 Acc: 0.6554\n",
      "val Loss: 0.6219 Acc: 0.6657\n",
      "Epoch 8...\n",
      "train Loss: 0.6232 Acc: 0.6436\n",
      "val Loss: 0.6329 Acc: 0.6370\n",
      "Epoch 9...\n",
      "train Loss: 0.6229 Acc: 0.6481\n",
      "val Loss: 0.6354 Acc: 0.6300\n",
      "Epoch 10...\n",
      "train Loss: 0.6169 Acc: 0.6655\n",
      "val Loss: 0.6009 Acc: 0.6776\n",
      "Epoch 11...\n",
      "train Loss: 0.6111 Acc: 0.6645\n",
      "val Loss: 0.6402 Acc: 0.6230\n",
      "Epoch 12...\n",
      "train Loss: 0.5990 Acc: 0.6736\n",
      "val Loss: 0.5815 Acc: 0.6910\n",
      "Epoch 13...\n",
      "train Loss: 0.6005 Acc: 0.6749\n",
      "val Loss: 0.5941 Acc: 0.6861\n",
      "Epoch 14...\n",
      "train Loss: 0.6008 Acc: 0.6676\n",
      "val Loss: 0.5642 Acc: 0.7183\n",
      "Epoch 15...\n",
      "train Loss: 0.5909 Acc: 0.6736\n",
      "val Loss: 0.5635 Acc: 0.7246\n",
      "Epoch 16...\n",
      "train Loss: 0.5790 Acc: 0.6843\n",
      "val Loss: 0.5557 Acc: 0.7183\n",
      "Epoch 17...\n",
      "train Loss: 0.5842 Acc: 0.6758\n",
      "val Loss: 0.6014 Acc: 0.6833\n",
      "Epoch 18...\n",
      "train Loss: 0.5742 Acc: 0.6906\n",
      "val Loss: 0.5646 Acc: 0.7092\n",
      "Epoch 19...\n",
      "train Loss: 0.5754 Acc: 0.6940\n",
      "val Loss: 0.5660 Acc: 0.7113\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50 \n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch {}...\".format(epoch))\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        corrects = 0\n",
    "\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(image_datasets[phase])\n",
    "        epoch_acc = corrects.double() / len(image_datasets[phase])\n",
    "\n",
    "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
